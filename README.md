# ğŸ¶ Music of the Day

**Music of the Day** is an end-to-end generative system that transforms the *semantic state of daily news* into an expressive ensemble composition.

Each day, the project ingests real-world events, analyzes their meaning and dynamics, and produces a unique piece of music that reflects how the world feels *today* â€” while remembering how it felt *yesterday*.

> *A living musical diary of global semantics.*

---

## âœ¨ What It Does

On every run, the system:

1. ğŸ“° Fetches daily news from configurable RSS sources  
2. ğŸ§  Embeds and analyzes semantic meaning  
3. ğŸ“ˆ Tracks change, novelty, and narrative momentum over time  
4. ğŸ¼ Maps semantic features to musical intent  
5. ğŸ¹ Composes expressive music (MIDI)  
6. ğŸ”Š Renders high-quality audio (WAV)  
7. ğŸ“ Writes a natural-language explanation of the result  

All in **one command**.

---

## ğŸ§  Architecture Overview
```bash
News â†’ Embeddings â†’ Semantic Features â†’ Musical Intent â†’ MIDI â†’ WAV
                  â†‘
                  Temporal Memory
```

---

## ğŸ—‚ï¸ Repository Structure
```bash
music-of-the-day/
â”œâ”€â”€ assets/
â”‚ â””â”€â”€ soundfonts/ # SoundFont for WAV rendering
â”œâ”€â”€ configs/
â”‚ â”œâ”€â”€ sources.yaml # RSS feeds & ingestion config
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ YYYY-MM-DD/
â”‚ â”œâ”€â”€ music.mid
â”‚ â”œâ”€â”€ music.wav
â”‚ â””â”€â”€ explanation.txt
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ run_daily.py # One-command daily runner
â”œâ”€â”€ src/music_of_the_day/
â”‚ â”œâ”€â”€ ingestion/ # News fetching & normalization
â”‚ â”œâ”€â”€ semantics/ # Embeddings, features, memory
â”‚ â”œâ”€â”€ mapping/ # Semantics â†’ music
â”‚ â”œâ”€â”€ music/ # MIDI + WAV generation
â”‚ â”œâ”€â”€ explain/ # Textual explanation
â”‚ â””â”€â”€ pipeline.py # Orchestration
â”œâ”€â”€ tests/ # Unit tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---

## ğŸ§  Semantic Layer

### News Ingestion
- Pulls articles via RSS feeds (configured in `configs/sources.yaml`)
- Normalizes and cleans text
- Designed for extensibility (additional sources, APIs)

### Embeddings
- Uses transformer-based sentence embeddings (e.g. **All-mpnet-base-v2**)
- Produces per-article vectors
- Aggregates into a **daily semantic embedding**

### Semantic Features

The system extracts interpretable features such as:

- **Semantic shift** â€“ how much today diverges from recent history  
- **Semantic novelty** â€“ how unusual todayâ€™s topics are  
- **Topic structure**
  - Number of topics
  - Topic dominance
  - Topic entropy  
- **Intra-day dispersion** â€“ diversity within todayâ€™s news  
- **Semantic velocity** â€“ day-over-day movement  
- **Semantic acceleration** â€“ change in velocity  
- **Narrative phase** â€“ inferred global state  
  (`build_up`, `climax`, `aftermath`, `stasis`)

### Temporal Memory
- Daily embeddings are persisted
- Rolling averages supported
- Semantic velocity stored across days
- Enables continuity and long-form evolution

---

## ğŸ¼ Semantics â†’ Music Mapping

Semantic features are translated into **musical intent**, expressed as high-level forces that guide composition rather than fixed musical facts:

- **Harmonic color** (bright / dark / ambiguous)
- **Base tempo** (global pacing anchor)
- **Dynamic intensity curve** (energy over time)
- **Tension curve** (harmonic and emotional pressure)
- **Texture density curve** (orchestration thickness)
- **Emotional vector** *(valence, arousal, tension)*
- **Motion profile** (`drift`, `rise`, `wave`, `collapse`)
- **Duration** (overall temporal scale)

Instead of prescribing notes or keys directly, this layer shapes how the music *behaves* over timeâ€”serving as the creative bridge between semantic meaning and audible form.

---

## ğŸ¹ Music Generation

### Composition

Music is generated from `MusicIntent` using **instrument-specific renderers** built on `pretty_midi`.  
Rather than a single solo instrument, the system produces a small **ensemble texture** driven by shared semantic curves.

- Intent-driven MIDI generation using `pretty_midi`
- Multi-instrument ensemble:
  - Piano
  - Strings
  - Bass
  - Percussion
- Time-discretized rendering over semantic frames
- Probabilistic note triggering based on texture density
- Expressive control derived from intent curves:
  - **Intensity curve** â†’ velocity and energy
  - **Density curve** â†’ note activation probability
  - **Tension curve** â†’ harmonic and registral pressure
  - **Motion profile** â†’ long-range musical behavior

Musical techniques include:
- Curve-shaped dynamics over time
- Density-weighted texture emergence
- Energy-driven harmonic tension
- Narrative-aware motion (`drift`, `rise`, `wave`, `collapse`)
- Stochastic variation for organic output

---

## â–¶ï¸ Running the Project

### Setup

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
python scripts/run_daily.py
```

---

Ensure you have FluidSynth installed and accessible from PATH.

---

## ğŸ§ª Tests

Run the full test suite:

```bash
pytest
```

Includes tests for:

- Semantic feature extraction

- News ingestion (mocked)

- Music generation

---

## ğŸ” Automation

The project is designed to support:

- Daily scheduled runs (e.g. GitHub Actions)

- Artifact uploads (MIDI, WAV, explanations)

- Long-term semantic and musical continuity

---

## ğŸ¨ Philosophy

### Music of the Day is not about turning news headlines into literal sounds.

Itâ€™s about capturing the motion of meaning â€”
how ideas shift, collide, accelerate, and settle â€”
and letting that motion leave a trace in music.

Some days are calm. Some days are tense.
This system listens â€” and plays.

--- 

## ğŸ“œ License

MIT License.
Use freely, remix boldly, credit kindly.

